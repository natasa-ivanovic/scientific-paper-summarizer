In a sequence-to-sequence (Seq2Seq) architecture, the encoder and decoder play different roles. The encoder's job is to encode the input sequence into a fixed-length representation (usually a context vector), while the decoder's job is to generate the output sequence from that encoded representation. As a result, their embeddings can be treated differently, and that's why they are initialized differently in your code.

Encoder Embedding:

In the encoder part of the Seq2Seq model, you are using a pre-trained word embedding matrix loaded from an external source. This is because the encoder is responsible for processing the input text, and you want it to leverage pre-trained word embeddings, which may capture semantic information about the words in your input sequence. By using pre-trained embeddings, the encoder can benefit from the knowledge contained in those embeddings, which is especially useful if the encoder and pre-trained embeddings have been trained on similar or related text data. So, you want to keep these embeddings fixed (trainable=False) to preserve their knowledge during training.
Decoder Embedding:

In the decoder part, the situation is different. The decoder's primary role is to generate the output sequence, and it doesn't necessarily need to rely on pre-trained embeddings for word representation. Instead, it's common to initialize the decoder's embeddings randomly. During training, the decoder will learn the embeddings that work best for the specific task of generating the output sequence. In this case, the embeddings are trainable, and the model learns the word representations that are most suitable for generating the target sequences.
So, to summarize, you use pre-trained embeddings in the encoder to take advantage of the knowledge encoded in those embeddings, while you initialize the decoder's embeddings from scratch because it's responsible for generating the output and can learn the word representations that work best for the task. This approach is common in Seq2Seq models and is suitable for various natural language processing tasks, such as machine translation and text summarization.